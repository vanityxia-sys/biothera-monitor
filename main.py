import feedparser
import requests
import os
import json
import time
from datetime import datetime, timedelta
from time import mktime

# --- 1. åŸºç¡€é…ç½® ---
# ç›‘æ§å¯¹è±¡
BASIC_KEYWORDS = ['"Bio-Thera Solutions"', '"ç™¾å¥¥æ³°"', '"688177"', 'Bio-Thera']

# æ—¶é—´é™åˆ¶ï¼šæ”¹ä¸º 365 å¤© (1å¹´)
DAYS_LIMIT = 365

# --- 2. æ™ºèƒ½åˆ†ç±»è¯åº“ ---

# Aç±»ï¼šä¸´åºŠä¸ç ”å‘ (æœ€æ ¸å¿ƒ)
CLINICAL_KEYWORDS = [
    "Clinical", "Trial", "Phase 1", "Phase 2", "Phase 3", "Phase I", "Phase II", "Phase III",
    "FDA", "NMPA", "EMA", "IND", "NDA", "BLA", "Approved", "Approval", "Study", "Endpoint",
    "R&D", "Pipeline", "Biosimilar", "Met primary endpoint",
    "ä¸´åºŠ", "è¯•éªŒ", "ä¸€æœŸ", "äºŒæœŸ", "ä¸‰æœŸ", "è·æ‰¹", "å—ç†", "è¯ç›‘å±€", 
    "æ•°æ®", "ç»ˆç‚¹", "å…¥ç»„", "é¦–ä¾‹", "ç ”å‘", "ç®¡çº¿", "ç”Ÿç‰©ç±»ä¼¼è¯"
]

# Bç±»ï¼šå•†ä¸šåŒ–ã€é”€å”®ä¸åˆä½œä¼™ä¼´ (ä½ ç‰¹åˆ«å…³å¿ƒçš„)
# åŒ…å«ä¸»è¦åˆä½œä¼™ä¼´ï¼šOrganon, Hikma, Biogen, Sandoz, Cipla, Intas ç­‰
COMMERCIAL_KEYWORDS = [
    "Sales", "Revenue", "Commercial", "Commercialization", "Launch", "Market", 
    "Agreement", "Partnership", "License", "Milestone", "Royalty", "Earnings", "Financial",
    "Organon", "Hikma", "Biogen", "Sandoz", "Cipla", "Intas", "Pharmapark", "SteinCares",
    "Tocilizumab", "Ustekinumab", "Avzivi", "Tofidence", "Pobevcy",  "Gedeon", "Stada", "Steincares",# æ ¸å¿ƒè¯ç‰©å
    "é”€å”®", "è¥æ”¶", "å•†ä¸šåŒ–", "ä¸Šå¸‚", "å¸‚åœº", "åˆä½œ", "åè®®", "æˆæƒ", 
    "é‡Œç¨‹ç¢‘", "é¦–ä»˜", "ç‰¹è®¸æƒ", "è´¢æŠ¥", "ä¸šç»©", "æ¬§åŠ éš†", "ç™¾å¥", "å±±å¾·å£«"ï¼Œ"å±±å¾·å£«",
    "BAT1406","BAT2094","BAT5906","BAT4406F","BAT1706","BAT1806","BAT2206","BAT2306","BAT2406","BAT2506","BAT2606",
]

BARK_KEY = os.environ.get("BARK_KEY")
HISTORY_FILE = "history.json"

def get_google_news():
    """è·å– Google News RSS æ•°æ®"""
    base_query = " OR ".join(BASIC_KEYWORDS)
    # æ‰©å¤§æœç´¢æ—¶é—´èŒƒå›´åˆ° 1 å¹´ (when:1y)
    query = f"({base_query}) when:1y"
    
    encoded_query = requests.utils.quote(query)
    # hl=en-US&gl=US ç¡®ä¿èƒ½æœåˆ°æµ·å¤–åˆä½œä¼™ä¼´(Hikma/Organon)å‘å¸ƒçš„è‹±æ–‡é€šç¨¿
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    
    print(f"æ­£åœ¨æŠ“å– RSS (è¿‡å»1å¹´): {rss_url}")
    return feedparser.parse(rss_url)

def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_history(history):
    # å› ä¸ºæ—¶é—´è·¨åº¦å¤§ï¼Œä¿ç•™æœ€è¿‘ 200 æ¡è®°å½•é˜²æ­¢é‡å¤
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history[-200:], f, ensure_ascii=False, indent=2)

def classify_news(title):
    """æ™ºèƒ½åˆ¤æ–­æ–°é—»ç±»åˆ«"""
    title_lower = title.lower()
    
    # ä¼˜å…ˆåˆ¤æ–­ä¸´åºŠ (é€šå¸¸ä¸´åºŠæ¶ˆæ¯å¯¹è‚¡ä»·å½±å“æœ€ç›´æ¥)
    for kw in CLINICAL_KEYWORDS:
        if kw.lower() in title_lower:
            return "clinical"
            
    # å…¶æ¬¡åˆ¤æ–­å•†ä¸š/åˆä½œ
    for kw in COMMERCIAL_KEYWORDS:
        if kw.lower() in title_lower:
            return "commercial"
            
    return "general"

def send_bark(title, url, date_str, news_type):
    """æ ¹æ®æ–°é—»ç±»åˆ«å‘é€ä¸åŒæ ·å¼çš„é€šçŸ¥"""
    if not BARK_KEY:
        return
    
    base_url = f"https://api.day.app/{BARK_KEY}/"
    
    # --- è§†è§‰ä¸å£°éŸ³åŒºåˆ† ---
    if news_type == "clinical":
        header = "ğŸ§¬ ç™¾å¥¥æ³°ä¸´åºŠè¿›å±•"
        body = f"**[ç ”å‘é‡ç£…]** {title}\n{date_str}"
        group = "BioThera-Clinical"
        sound = "glass" # æ¸…è„†æç¤ºéŸ³
        icon = "https://cdn-icons-png.flaticon.com/512/2965/2965536.png" # DNAå›¾æ ‡
        
    elif news_type == "commercial":
        header = "ğŸ’° ç™¾å¥¥æ³°å•†ä¸šåŠ¨æ€"
        body = f"**[åˆä½œ/é”€å”®]** {title}\n{date_str}"
        group = "BioThera-Commercial"
        sound = "chime" # æ‚¦è€³æç¤ºéŸ³
        icon = "https://cdn-icons-png.flaticon.com/512/2454/2454282.png" # é’±è¢‹/æ¡æ‰‹å›¾æ ‡
        
    else:
        header = "ğŸ“° ç™¾å¥¥æ³°æ—¥å¸¸èµ„è®¯"
        body = f"{title}\n{date_str}"
        group = "BioThera-General"
        sound = "minuet" # ä½è°ƒæç¤ºéŸ³
        icon = "https://www.bio-thera.com/favicon.ico"

    print(f"æ­£åœ¨æ¨é€ [{news_type}]: {title}")
    
    try:
        requests.post(base_url, data={
            "title": header,
            "body": body,
            "url": url,
            "group": group,
            "level": "active", # å‡ä¸ºä¸»åŠ¨æé†’
            "sound": sound,
            "icon": icon
        })
    except Exception as e:
        print(f"æ¨é€å¤±è´¥: {e}")

def main():
    feed = get_google_news()
    history = load_history()
    seen_links = {item['link'] for item in history}
    
    new_items = []
    cutoff_date = datetime.now() - timedelta(days=DAYS_LIMIT)
    print(f"è¿‡æ»¤æ—¶é—´æˆªæ­¢çº¿: {cutoff_date.strftime('%Y-%m-%d')}")

    # å€’åºå¤„ç†ï¼Œç¡®ä¿æ—§æ–°é—»å…ˆå…¥åº“
    for entry in feed.entries[::-1]:
        link = entry.link
        title = entry.title
        
        # æ—¶é—´è§£æ
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            pub_dt = datetime.fromtimestamp(mktime(entry.published_parsed))
        else:
            pub_dt = datetime.now()

        # ä¸¥æ ¼çš„æ—¶é—´è¿‡æ»¤
        if pub_dt < cutoff_date:
            continue

        if link not in seen_links:
            # åˆ†ç±»
            news_type = classify_news(title)
            
            # æ¨é€
            send_bark(title, link, entry.published, news_type)
            
            new_items.append({
                "title": title, 
                "link": link, 
                "date": entry.published,
                "type": news_type
            })
            seen_links.add(link)
            # ç¨å¾®åœé¡¿ï¼Œé¿å…ç¬æ—¶è¯·æ±‚è¿‡å¤š
            time.sleep(1)
    
    if new_items:
        history.extend(new_items)
        save_history(history)
        print(f"å¤„ç†å®Œæˆï¼Œæ–°å¢ {len(new_items)} æ¡ã€‚")
    else:
        print("æš‚æ— æ–°æ¶ˆæ¯ã€‚")

if __name__ == "__main__":
    main()
